{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the following libraries\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dropout, Input, Conv2D, MaxPooling2D, Flatten\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras as k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Load Dataset\n",
    "We will be using the MNIST dataset which contains photos of handwritten digits. Keras already comes with this dataset, so we can load it directly with library functions as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Inspect Data\n",
    "Start by inspecting the data. Write code to output the following: \n",
    "- The shape of the training data\n",
    "- The shape of the testing data\n",
    "- The total number of outputs\n",
    "- A lit of the output classes\n",
    "- Display the first image in the training data\n",
    "- Display the first image in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape : 47040000\n",
      "Testing data shape : 7840000\n",
      "Total number of outputs : 70000\n",
      "Output classes : [5, 0, 4, 1, 9, 2, 3, 6, 7, 8]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b183cf1f08>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### ======== YOUR CODE HERE ========== ###\n",
    "\n",
    "# Print shape of training data\n",
    "print('Training data shape : {}'.format(train_images.size))\n",
    " \n",
    "# Print shape of testing data    \n",
    "print('Testing data shape : {}'.format(test_images.size))\n",
    " \n",
    "# Print the total number of outputs\n",
    "print('Total number of outputs : {}'.format(train_labels.size+test_labels.size))\n",
    "\n",
    "# Print the list of output classes\n",
    "uni = []\n",
    "for lb in train_labels:\n",
    "    if lb not in uni:\n",
    "        uni.append(lb)\n",
    "    \n",
    "\n",
    "print('Output classes : {}'.format(uni))\n",
    " \n",
    "# Display the first image in the training data    \n",
    "\n",
    "plt.imshow(train_images[0], cmap = 'gray')\n",
    "    \n",
    "# Display the first image in the testing data  \n",
    "\n",
    "plt.imshow(test_images[0], cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 - Preprocess Data\n",
    "As usual, we need to preprocess our data. Write code to do the following:\n",
    "- Reshape the images from a 28x28 matrix to 784 flattend array (28x28=784), so it can be fed into the network as a single feature\n",
    "- Convert the datatype to 'float32' and then normalie the pixels so the values range between 0 and 1 (Hint: The current values range from 0 to 255)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "input_shape = (28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = train_images\n",
    "# X_test = test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to data type to float32\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data to a scale between 0 and 1\n",
    "X_train = X_train/255.0\n",
    "X_test = X_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [1.80925895e-07],\n",
       "        [1.08555537e-06],\n",
       "        [1.08555537e-06],\n",
       "        [1.08555537e-06],\n",
       "        [7.59888735e-06],\n",
       "        [8.20197420e-06],\n",
       "        [1.05540103e-05],\n",
       "        [1.56802435e-06],\n",
       "        [1.00112329e-05],\n",
       "        [1.53787023e-05],\n",
       "        [1.48962317e-05],\n",
       "        [7.65919594e-06],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [1.80925895e-06],\n",
       "        [2.17111074e-06],\n",
       "        [5.66901099e-06],\n",
       "        [9.28752979e-06],\n",
       "        [1.02524673e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.35694418e-05],\n",
       "        [1.03730845e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.45946888e-05],\n",
       "        [1.17601830e-05],\n",
       "        [3.85975272e-06],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [2.95512291e-06],\n",
       "        [1.43534544e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.51374661e-05],\n",
       "        [5.60870239e-06],\n",
       "        [4.94530786e-06],\n",
       "        [4.94530786e-06],\n",
       "        [3.37728329e-06],\n",
       "        [2.35203674e-06],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [1.08555537e-06],\n",
       "        [1.32075902e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.19411088e-05],\n",
       "        [1.09761704e-05],\n",
       "        [1.48962317e-05],\n",
       "        [1.45343802e-05],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [4.82469068e-06],\n",
       "        [9.40814698e-06],\n",
       "        [6.45302362e-06],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.23632690e-05],\n",
       "        [6.63394928e-07],\n",
       "        [0.00000000e+00],\n",
       "        [2.59327112e-06],\n",
       "        [9.28752979e-06],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [8.44320823e-07],\n",
       "        [6.03086363e-08],\n",
       "        [9.28752979e-06],\n",
       "        [1.52580833e-05],\n",
       "        [5.42777661e-06],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [8.38289998e-06],\n",
       "        [1.52580833e-05],\n",
       "        [1.14586392e-05],\n",
       "        [1.20617273e-07],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [6.63394928e-07],\n",
       "        [1.14586392e-05],\n",
       "        [1.52580833e-05],\n",
       "        [4.22160429e-06],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [2.11080214e-06],\n",
       "        [1.45343802e-05],\n",
       "        [1.35694418e-05],\n",
       "        [9.64938135e-06],\n",
       "        [6.51333221e-06],\n",
       "        [6.03086363e-08],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [4.88499927e-06],\n",
       "        [1.44740716e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [7.17672719e-06],\n",
       "        [1.50771575e-06],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [2.71388831e-06],\n",
       "        [1.12174048e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [9.04629542e-06],\n",
       "        [1.62833305e-06],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [9.64938181e-07],\n",
       "        [5.60870239e-06],\n",
       "        [1.51977747e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.12777134e-05],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [1.50168489e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.50168489e-05],\n",
       "        [3.85975272e-06],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [2.77419690e-06],\n",
       "        [7.84012263e-06],\n",
       "        [1.10364790e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.24838862e-05],\n",
       "        [1.20617273e-07],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [2.35203674e-06],\n",
       "        [8.92567732e-06],\n",
       "        [1.38106761e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.50771575e-05],\n",
       "        [1.09761704e-05],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [1.44740716e-06],\n",
       "        [6.87518377e-06],\n",
       "        [1.33282074e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.21220346e-05],\n",
       "        [4.70407349e-06],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [1.38709845e-06],\n",
       "        [3.98036991e-06],\n",
       "        [1.28457386e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.19411088e-05],\n",
       "        [4.88499927e-06],\n",
       "        [1.20617273e-07],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [1.08555537e-06],\n",
       "        [1.03127759e-05],\n",
       "        [1.32075902e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.17601830e-05],\n",
       "        [4.82469068e-06],\n",
       "        [5.42777684e-07],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [3.31697470e-06],\n",
       "        [1.03730845e-05],\n",
       "        [1.36297504e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.47153060e-05],\n",
       "        [8.02104842e-06],\n",
       "        [6.63394928e-07],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [8.20197420e-06],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.52580833e-05],\n",
       "        [1.27854291e-05],\n",
       "        [8.14166560e-06],\n",
       "        [7.96073982e-06],\n",
       "        [9.64938181e-07],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00]],\n",
       "\n",
       "       [[0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00],\n",
       "        [0.00000000e+00]]], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 - Convert Labels \n",
    "Conver the labels from integer to categorical (one-hot encoding). We have to do this conversion, because that is the format required by Keras to perform multiclass classification. One-hot encoding converts the integer to an array of all zeros except a 1 at the index of the integer.\n",
    "\n",
    "For example, using a one-hot encoding for 10 classes, the integer 5 will be encoded as 0000010000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### ======== YOUR CODE HERE ========== ###\n",
    "# Change the labels from integer to categorical data and store this in a new variable \n",
    "# (Hint: Use the to_categorical function in keras)\n",
    "\n",
    "y_train = to_categorical(train_labels,10)\n",
    "y_test = to_categorical(test_labels,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the original and converted labels the item in the dataset\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 - Create model\n",
    "\n",
    "Create a sequential model with the following architecture:\n",
    "- an input dense layer of 512 units using the ReLU activation function, with input dimension of 784\n",
    "- a dense layer of 512 units with the ReLU activation function\n",
    "- a output layer of 10 units with the softmax activation function (output layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cell to get rid of some warnings later on\n",
    "\n",
    "def noWarnings():\n",
    "    import os\n",
    "    os.environ['TF_CPP_MIN_VLOG_LEVEL'] = '0'\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\n",
    "    from tensorflow import logging\n",
    "    logging.set_verbosity(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ======== YOUR CODE HERE ========== ###\n",
    "\n",
    "# Create sequential model\n",
    "noWarnings()\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', \n",
    "                 input_shape=input_shape,name='input'))\n",
    "\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', \n",
    "                 input_shape=input_shape))\n",
    "\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512,input_shape = (784,),activation = 'relu'))\n",
    "model.add(Dense(512,activation = 'relu'))\n",
    "model.add(Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 - Compile Model\n",
    "\n",
    "Compile the model with an **rmsprop optimizer, categorical_crossentropy loss, and accuracy metrics**. You can try other optimizers too such as sgd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7 - Train model\n",
    "Fit the model and train for **20 epochs** and a **batch size of 256**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0829 21:10:32.667521  9480 deprecation.py:323] From c:\\users\\bp\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0829 21:10:32.719421  9480 deprecation_wrapper.py:119] From c:\\users\\bp\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:766: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0829 21:10:32.727410  9480 deprecation.py:506] From c:\\users\\bp\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:519: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 27s - loss: 2.3017 - acc: 0.1120    \n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 28s - loss: 2.3014 - acc: 0.1124    \n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 30s - loss: 2.3014 - acc: 0.1124    \n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 34s - loss: 2.3013 - acc: 0.1124    \n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 33s - loss: 2.3013 - acc: 0.1124    \n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 29s - loss: 2.3013 - acc: 0.1124    \n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 30s - loss: 2.3013 - acc: 0.1124    \n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 29s - loss: 2.3013 - acc: 0.1124    \n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 29s - loss: 2.3013 - acc: 0.1124    \n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 29s - loss: 2.3013 - acc: 0.1124    \n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 29s - loss: 2.3013 - acc: 0.1124    \n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 29s - loss: 2.3013 - acc: 0.1124    \n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 29s - loss: 2.3013 - acc: 0.1124    \n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 29s - loss: 2.3012 - acc: 0.1124    \n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 31s - loss: 2.3013 - acc: 0.1124    \n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 32s - loss: 2.3013 - acc: 0.1124    \n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 30s - loss: 2.3013 - acc: 0.1124    \n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 33s - loss: 2.3013 - acc: 0.1124    \n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 31s - loss: 2.3013 - acc: 0.1124    \n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 28s - loss: 2.3012 - acc: 0.1124    \n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, verbose=True, epochs=20, batch_size = 256)\n",
    "model.save('./stuff.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ======== YOUR CODE HERE ========== ###\n",
    "from keras.models import load_model\n",
    "#Load model\n",
    "model = load_model('.\\stuff.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8 - Evaluate model\n",
    "Report the loss and accuracy on the test data. (Hint: Use the built in *model.evaluate* function in keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9920/10000 [============================>.] - ETA: 0sTest loss: 2.301043939971924\n",
      "Test accuracy: 0.1135\n"
     ]
    }
   ],
   "source": [
    "### ======== YOUR CODE HERE ========== ###\n",
    "\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Report loss and accuracy on test data\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9 - Plot results\n",
    "- Plot the loss curves for both training and validation\n",
    "- Plot the accuracy curves for both training and validation\n",
    "\n",
    "Hint: Use the *.history* function to access these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b194ca0448>]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### ======== YOUR CODE HERE ========== ###\n",
    "\n",
    "# Plot the Loss Curves\n",
    "\n",
    "plt.plot(history.history['loss'])    \n",
    "     \n",
    "# Plot the Accuracy Curves\n",
    "plt.plot(history.history['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 - Regularization\n",
    "If done properly, you will notice there is some overfitting in the model. To overcome this, we will use a regularization method called **dropout** which you will learn in the CNN lecture. This requires simply adding dropout layers into our model. To do so, repeat steps 5-8 with the following architecture.\n",
    "- an input dense layer of 512 units using the ReLU activation function, with input dimension of 784\n",
    "- a dropout layer with 0.5 dropout rate\n",
    "- a dense layer of 512 units with the ReLU activation function\n",
    "- a dropout layer with 0.5 dropout rate\n",
    "- a output layer of 10 units with the softmax activation function (output layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ======== YOUR CODE HERE ========== ###\n",
    "\n",
    "# Repeat step 5 with new architecture\n",
    "new_model = Sequential()\n",
    "new_model.add(Dense(512,input_shape = (784,), activation = 'relu'))\n",
    "new_model.add(Dropout(0.5))\n",
    "new_model.add(Dense(512, activation = 'relu'))\n",
    "new_model.add(Dropout(0.5))\n",
    "new_model.add(Dense(10, activation = 'softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat step 6 with new model\n",
    "new_model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.4028 - acc: 0.8763\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 5s 87us/step - loss: 0.1707 - acc: 0.9477\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 5s 91us/step - loss: 0.1306 - acc: 0.9608: 0s - loss: 0.1307 - acc: 0.\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 6s 93us/step - loss: 0.1077 - acc: 0.9657\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 5s 91us/step - loss: 0.0922 - acc: 0.9710\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 5s 92us/step - loss: 0.0845 - acc: 0.9732\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 5s 87us/step - loss: 0.0747 - acc: 0.9769\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 5s 89us/step - loss: 0.0693 - acc: 0.9787\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 6s 92us/step - loss: 0.0643 - acc: 0.9786\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 5s 90us/step - loss: 0.0591 - acc: 0.9812\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.0554 - acc: 0.9824\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 6s 92us/step - loss: 0.0539 - acc: 0.9826\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 5s 90us/step - loss: 0.0532 - acc: 0.9826\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.0492 - acc: 0.9845\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 6s 92us/step - loss: 0.0471 - acc: 0.9850\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 5s 87us/step - loss: 0.0448 - acc: 0.9850\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 5s 87us/step - loss: 0.0440 - acc: 0.9856\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 6s 93us/step - loss: 0.0403 - acc: 0.9866\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 5s 87us/step - loss: 0.0414 - acc: 0.9863\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.0374 - acc: 0.9881\n"
     ]
    }
   ],
   "source": [
    "# Repeat step 7 with new model\n",
    "history = new_model.fit(X_train, y_train, verbose=True, epochs=20, batch_size = 256)\n",
    "model.save('./new_stuff.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 80us/step\n",
      "Test loss: 0.058619084055074926\n",
      "Test accuracy: 0.9841\n"
     ]
    }
   ],
   "source": [
    "# Repeat step 8 with new model\n",
    "### ======== YOUR CODE HERE ========== ###\n",
    "\n",
    "loss, acc = new_model.evaluate(X_test, y_test)\n",
    "\n",
    "# Report loss and accuracy on test data\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x258ef5427c8>]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Repeat step 9 with new model\n",
    "### ======== YOUR CODE HERE ========== ###\n",
    "\n",
    "# Plot the Loss Curves\n",
    "\n",
    "plt.plot(history.history['loss'])    \n",
    "     \n",
    "# Plot the Accuracy Curves\n",
    "plt.plot(history.history['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BONUS QUESTION - Modified MNIST\n",
    "The modified MNIST dataset consists of images that contain multiple written digits with background noise. The labels for this dataset is the largest digit contained in the image. Your challenge will be to train a feed forward network to predict these labels. You can follow the same steps as in this assignment to create the feed forward neural network, but you will likely have to experiment with different data preprocessing techniques and network structures. You will also need to load the dataset yourself and put it into a form that is acceptable by keras.\n",
    "\n",
    "The dataset can be downloaded from (https://techx.blob.core.windows.net/modified-mnist/modified-mnist.zip)[here]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
